{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "simulation_root = \"/Users/user_name/Library/Application Support/DefaultCompany/Sumobot/Logs/Batch/20251231_170916_batch\"\n",
    "\n",
    "converted_dir = \"converted\"\n",
    "summarized_dir = \"result\"\n",
    "batch_checkpoint_dir = \"batched\"\n",
    "arena_heatmaps_output = \"result/arena_heatmaps\"\n",
    "\n",
    "os.makedirs(converted_dir, exist_ok=True)\n",
    "os.makedirs(batch_checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(summarized_dir, exist_ok=True)\n",
    "os.makedirs(arena_heatmaps_output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Simulation Log to Parquet / CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compile.log_to_parquet import ( \n",
    "    convert_all_configs\n",
    ")\n",
    "\n",
    "convert_all_configs(simulation_root, converted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from compile.log_to_csv import ( \n",
    "#     convert_all_configs\n",
    "# )\n",
    "\n",
    "# convert_all_configs(simulation_root, converted_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summarization CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compile.generator import (\n",
    "        batch_process_csvs,\n",
    "        generate_timebins_from_batches,\n",
    "        generate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Batched CSV\n",
    "\n",
    "Process CSVs in batches and save checkpoints\n",
    "\n",
    "Structure: base_dir/BotA_vs_BotB/ConfigFolder/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timebin_size = 5\n",
    "batch_size = 2 # if there's 156 matchup simulation folder, it will generate 156 / 2 = 78 summarization batch csv\n",
    "input_format = \"auto\"  # \"csv\", \"parquet\", or \"auto\" (auto prefers parquet over csv)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "batch_process_csvs(\n",
    "    converted_dir, \n",
    "    batch_size=batch_size,\n",
    "    time_bin_size=timebin_size,\n",
    "    checkpoint_dir=batch_checkpoint_dir,\n",
    "    compute_timebins=True,\n",
    "    input_format=input_format)\n",
    "\n",
    "elapsed_seconds = time.time() - start\n",
    "hours, remainder = divmod(elapsed_seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "processing_time = f\"{int(hours):02d}:{int(minutes):02d}:{seconds:.2f}\"\n",
    "print(f\"\\nProcessing Time: {processing_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Final Summarization CSV from Batches\n",
    "\n",
    "Generate timebin summaries from batched timebin checkpoints\n",
    "Loads batch files and creates final summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(batch_checkpoint_dir, summarized_dir) # generate summarization csv\n",
    "generate_timebins_from_batches(batch_checkpoint_dir, summarized_dir) # generate csv containing batched timebins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Arena Heatmaps Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from compile.arena_generator import ( \n",
    "    create_phased_heatmaps_all_bots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run to Generate Arena Heatmap figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "create_phased_heatmaps_all_bots(\n",
    "            converted_dir,\n",
    "            output_dir = arena_heatmaps_output,\n",
    "            actor_position=\"both\",\n",
    "            chunksize=50000,\n",
    "            max_configs=None,  # None for all configs, only fill to test, e.g. 2 or 5 configs\n",
    "            mode=\"all\",  # Generate both heatmaps and position distributions\n",
    "            use_timer=False, # Group by existing Timer configuration\n",
    "            use_time_windows=True, # Use time windows [skip_initial-15, 15-30, 30-45, 45-60]\n",
    "            include_distance_over_time=True,  \n",
    "            skip_initial=2.5\n",
    "        )\n",
    "\n",
    "elapsed_seconds = time.time() - start\n",
    "hours, remainder = divmod(elapsed_seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "processing_time = f\"{int(hours):02d}:{int(minutes):02d}:{seconds:.2f}\"\n",
    "print(f\"\\nProcessing Time: {processing_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from plotting.overall_analyzer import (\n",
    "        plot_action_radar,\n",
    "        plot_collision_radar,\n",
    "        plot_winrate_matrix,\n",
    "        plot_overall_bot_metrics,\n",
    "        plot_grouped_config_winrates,\n",
    "        plot_time_related,\n",
    "        plot_action_distribution_stacked,\n",
    "        plot_action_timebins_intensity,\n",
    "        plot_collision_timebins_intensity,\n",
    "        plot_collision_distribution_stacked,\n",
    "        plot_action_win_related,\n",
    "        plot_all_correlations,\n",
    "        plot_full_cross_heatmap_half,\n",
    "    )\n",
    "    \n",
    "from plotting.individual_analyzer import (\n",
    "    plot_individual_bot_correlations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary data\n",
    "df_sum = pd.read_csv(f\"{summarized_dir}/summary_bot.csv\").rename(columns={\"Duration\": \"Duration (ms)\"})\n",
    "df = pd.read_csv(f\"{summarized_dir}/summary_matchup.csv\")\n",
    "df_timebins = pd.read_csv(f\"{summarized_dir}/summary_action_timebins.csv\")\n",
    "df_collision_timebins = pd.read_csv(f\"{summarized_dir}/summary_collision_timebins.csv\")\n",
    "\n",
    "# Configuration\n",
    "cfg = {\n",
    "    \"Timer\": sorted(df[\"Timer\"].unique().tolist()),\n",
    "    \"ActInterval\": sorted(df[\"ActInterval\"].unique().tolist()),\n",
    "    \"Round\": sorted(df[\"Round\"].unique().tolist()),\n",
    "    \"SkillLeft\": sorted(df[\"SkillLeft\"].unique().tolist()),\n",
    "    \"SkillRight\": sorted(df[\"SkillRight\"].unique().tolist()),\n",
    "    \"Bots\": sorted(df[\"Bot_L\"].unique().tolist()),\n",
    "}\n",
    "bots = str.join(\", \", cfg[\"Bots\"])\n",
    "\n",
    "# Display settings\n",
    "width = 10\n",
    "height = 6\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"\\nBots in experiment: {bots}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Matchup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate experiment statistics\n",
    "num_configs = len(df)  # Each row is a unique configuration\n",
    "num_bots = len(cfg[\"Bots\"])\n",
    "num_matchups = len(df.groupby(['Bot_L', 'Bot_R']))\n",
    "total_games = df[\"Games\"].sum()\n",
    "\n",
    "# Calculate configuration combinations\n",
    "num_timer_configs = len(cfg[\"Timer\"])\n",
    "num_actinterval_configs = len(cfg[\"ActInterval\"])\n",
    "num_round_configs = len(cfg[\"Round\"])\n",
    "num_skill_configs = len(cfg[\"SkillLeft\"]) * len(cfg[\"SkillRight\"])\n",
    "total_config_combinations = num_timer_configs * num_actinterval_configs * num_round_configs * num_skill_configs\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of Bots: {num_bots}\")\n",
    "print(f\"Number of Unique Matchups: {num_matchups}\")\n",
    "print(f\"Number of Configuration Combinations: {total_config_combinations}\")\n",
    "print(f\"  - Timer variations: {num_timer_configs}\")\n",
    "print(f\"  - ActInterval variations: {num_actinterval_configs}\")\n",
    "print(f\"  - Round variations: {num_round_configs}\")\n",
    "print(f\"  - Skill combinations: {num_skill_configs}\")\n",
    "print(f\"Number of Unique Config-Matchup Pairs: {num_configs}\")\n",
    "print(f\"Total Games Played: {total_games}\")\n",
    "print(f\"Games per Config-Matchup: {df['Games'].iloc[0] if len(df) > 0 else 'N/A'}\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "# Show games played per bot\n",
    "print(\"Games Played per Bot:\")\n",
    "print(\"=\"*60)\n",
    "bot_game_counts = []\n",
    "for bot in sorted(df['Bot_L'].unique()):\n",
    "    # Count games where bot is on left side\n",
    "    games_left = df[df['Bot_L'] == bot]['Games'].sum()\n",
    "    # Count games where bot is on right side\n",
    "    games_right = df[df['Bot_R'] == bot]['Games'].sum()\n",
    "    total_games = games_left + games_right\n",
    "    bot_game_counts.append(total_games)\n",
    "    print(f\"{bot}: {total_games} games ({games_left} as Bot_L + {games_right} as Bot_R)\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "unique_games = df['Games'].sum()\n",
    "total_participations = sum(bot_game_counts)\n",
    "print(f\"Unique games in dataset: {unique_games} games\")\n",
    "print(f\"Total bot participations: {' + '.join(map(str, bot_game_counts))} = {total_participations}\")\n",
    "print()\n",
    "\n",
    "display(df_sum)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Analysis\n",
    "\n",
    "Analyze bot agents facing other agents with similar configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bot Behaviour Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions Behaviour\n",
    "Mean action counts per bot across all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: aggregated action counts from both sides (Bot_L)\n",
    "print(f\"Sample: {len(df)} config-matchup pairs (all bots combined)\")\n",
    "fig = plot_action_radar(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collision Behaviour\n",
    "Hit/Struck/Tie distribution per bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: aggregated collision counts from both sides (Bot_L)\n",
    "print(f\"Sample: {len(df)} config-matchup pairs (all bots combined)\")\n",
    "fig = plot_collision_radar(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Matrix\n",
    "\n",
    "Shows how often each bot wins against others across different matchups.\n",
    "This is calculated with taking mean of each configuration (10-games iteration matchup) resulting 240 games in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: mean winrate across all configurations per matchup\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df.groupby(['Bot_L', 'Bot_R']))} unique matchups\")\n",
    "fig = plot_winrate_matrix(df, width, height)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: mean action counts per bot across all configurations\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df['Bot_L'].unique())} bots\")\n",
    "fig = plot_overall_bot_metrics(df, metric=\"ActionCounts_L\", title=\"Mean Action per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: mean action duration per bot across all configurations\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df['Bot_L'].unique())} bots\")\n",
    "fig = plot_overall_bot_metrics(df, metric=\"Duration_L\", title=\"Mean Action Duration per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: mean collisions per bot across all configurations\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df['Bot_L'].unique())} bots\")\n",
    "fig = plot_overall_bot_metrics(df, metric=\"Collisions_L\", title=\"Mean Collisions per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: mean match duration per bot across all configurations\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df['Bot_L'].unique())} bots\")\n",
    "fig = plot_overall_bot_metrics(df, metric=\"MatchDur\", title=\"Mean Match Duration per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: winrate grouped by Timer\n",
    "timer_groups = df.groupby(['Timer', 'Bot_L']).size()\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(timer_groups)} (Timer × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: winrate grouped by ActInterval\n",
    "actint_groups = df.groupby(['ActInterval', 'Bot_L']).size()\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(actint_groups)} (ActInterval × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: winrate grouped by Round\n",
    "round_groups = df.groupby(['Round', 'Bot_L']).size()\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(round_groups)} (Round × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: winrate grouped by Skill (SkillLeft)\n",
    "skill_groups = df.groupby(['SkillLeft', 'Bot_L']).size()\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(skill_groups)} (Skill × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: collisions grouped by Timer\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(timer_groups)} (Timer × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: collisions grouped by ActInterval\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(actint_groups)} (ActInterval × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: collisions grouped by Round\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(round_groups)} (Round × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: collisions grouped by Skill\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(skill_groups)} (Skill × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action counts grouped by Timer\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(timer_groups)} (Timer × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action counts grouped by ActInterval\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(actint_groups)} (ActInterval × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action counts grouped by Round\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(round_groups)} (Round × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action counts grouped by Skill\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(skill_groups)} (Skill × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action duration grouped by Timer\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(timer_groups)} (Timer × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action duration grouped by ActInterval\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(actint_groups)} (ActInterval × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action duration grouped by Round\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(round_groups)} (Round × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action duration grouped by Skill\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(skill_groups)} (Skill × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: match duration grouped by Timer\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(timer_groups)} (Timer × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: match duration grouped by ActInterval\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(actint_groups)} (ActInterval × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: match duration grouped by Round\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(round_groups)} (Round × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: match duration grouped by Skill\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(skill_groups)} (Skill × Bot) groups\")\n",
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Related Trends\n",
    "\n",
    "Analyzes Bots aggressiveness over game duration with determining how much action taken duration related to the overall game duration (Time Setting).\n",
    "Higher timers don't always lead to longer matches. Some matchups finish fights early regardless of time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: match duration vs bot action duration (Timer × ActInterval × Bot groups)\n",
    "time_groups = df.groupby(['Timer', 'ActInterval', 'Bot_L']).size()\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(time_groups)} (Timer × ActInterval × Bot) groups\")\n",
    "figs = plot_time_related(df, width, height)\n",
    "for fig in figs:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Distribution per Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action type distribution per bot (all configurations aggregated)\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df['Bot_L'].unique())} bots\")\n",
    "fig = plot_action_distribution_stacked(df, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Intensity Over Time (Per Configuration)\n",
    "\n",
    "Shows action intensity over time for different timer and action interval configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timI in cfg[\"Timer\"]:\n",
    "    for actI in cfg[\"ActInterval\"]:\n",
    "        # Filter for specific config\n",
    "        filtered = df_timebins[(df_timebins['Timer'] == timI) & (df_timebins['ActInterval'] == actI)]\n",
    "        print(f\"\\n--- Timer={timI}, ActionInterval={actI} ---\")\n",
    "        print(f\"Sample: {len(filtered)} timebin records\")\n",
    "        \n",
    "        # Total action intensity\n",
    "        fig = plot_action_timebins_intensity(df_timebins, timer=timI, act_interval=actI, mode=\"total\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Per-action intensity\n",
    "        fig = plot_action_timebins_intensity(df_timebins, timer=timI, act_interval=actI, mode=\"per_action\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Intensity Over All Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: action intensity over time (all configurations with Timer=60)\n",
    "filtered = df_timebins[df_timebins['Timer'] == 60]\n",
    "print(f\"Sample: {len(filtered)} timebin records (Timer=60)\")\n",
    "fig = plot_action_timebins_intensity(df_timebins, mode=\"total\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: per-action intensity over time (all configurations with Timer=60)\n",
    "print(f\"Sample: {len(filtered)} timebin records (Timer=60)\")\n",
    "fig = plot_action_timebins_intensity(df_timebins, mode=\"per_action\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Intensity Over Time (Per Configuration)\n",
    "\n",
    "Shows collision intensity over time for different timer and action interval configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timI in cfg[\"Timer\"]:\n",
    "    for actI in cfg[\"ActInterval\"]:\n",
    "        # Filter for specific config\n",
    "        filtered = df_collision_timebins[(df_collision_timebins['Timer'] == timI) & (df_collision_timebins['ActInterval'] == actI)]\n",
    "        print(f\"\\n--- Timer={timI}, ActionInterval={actI} ---\")\n",
    "        print(f\"Sample: {len(filtered)} collision timebin records\")\n",
    "        \n",
    "        # Total collision intensity\n",
    "        fig = plot_collision_timebins_intensity(df_collision_timebins, timer=timI, act_interval=actI, mode=\"total\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Per-type collision intensity\n",
    "        fig = plot_collision_timebins_intensity(df_collision_timebins, timer=timI, act_interval=actI, mode=\"per_type\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Detail Distribution per Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: collision type distribution per bot (all configurations aggregated)\n",
    "print(f\"Sample: {len(df)} config-matchup pairs → {len(df['Bot_L'].unique())} bots\")\n",
    "fig = plot_collision_distribution_stacked(df, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Intensity Over All Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: collision intensity over time (all configurations with Timer=60)\n",
    "filtered = df_collision_timebins[df_collision_timebins['Timer'] == 60]\n",
    "print(f\"Sample: {len(filtered)} collision timebin records (Timer=60)\")\n",
    "fig = plot_collision_timebins_intensity(df_collision_timebins, mode=\"total\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: per-type collision intensity over time (all configurations with Timer=60)\n",
    "print(f\"Sample: {len(filtered)} collision timebin records (Timer=60)\")\n",
    "fig = plot_collision_timebins_intensity(df_collision_timebins, mode=\"per_type\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken vs. Win Relation\n",
    "\n",
    "Does spending most action (aggressive) lead to a win?\n",
    "This taking mean of action-taken per games versus win-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: average actions per game vs winrate (all bots, all configs)\n",
    "print(f\"Sample: {len(df)} config-matchup pairs (both Bot_L and Bot_R sides)\")\n",
    "fig = plot_action_win_related(df, width, height)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Analysis (Overall)\n",
    "\n",
    "Correlation analysis using Pearson coefficient with scatter plots and regression lines.\n",
    "All data from all bots combined, separated by configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: correlation analysis combines both Bot_L and Bot_R perspectives\n",
    "correlation_data_size = len(df) * 2  # Each config-matchup has 2 bot perspectives\n",
    "print(f\"Sample: {len(df)} config-matchup pairs × 2 perspectives = {correlation_data_size} data points\")\n",
    "correlation_figs = plot_all_correlations(df, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Bot Analysis\n",
    "\n",
    "Analyze bot agent against its different configurations.\n",
    "Each of report: Win Rate; Collision; Action-Taken; Duration; is calculated with averaging data from matchup (left and right position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Analysis (Per Bot)\n",
    "\n",
    "Detailed plots for individual bots, separated by configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique bots\n",
    "bots_list = sorted(df['Bot_L'].unique())\n",
    "print(f\"Analyzing {len(bots_list)} bots: {bots_list}\")\n",
    "\n",
    "# Individual bot correlation analysis\n",
    "for bot in bots_list:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing correlations for {bot}\")\n",
    "    \n",
    "    # Calculate sample size for this bot\n",
    "    bot_left = len(df[df['Bot_L'] == bot])\n",
    "    bot_right = len(df[df['Bot_R'] == bot])\n",
    "    bot_total = bot_left + bot_right\n",
    "    print(f\"Sample: {bot_left} as Bot_L + {bot_right} as Bot_R = {bot_total} data points\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    correlation_figs = plot_individual_bot_correlations(df, bot, width, height)\n",
    "    \n",
    "    if not correlation_figs:\n",
    "        print(f\"No data available for {bot}\")\n",
    "        continue\n",
    "    \n",
    "    # Win Rate vs ActInterval\n",
    "    if 'actinterval' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Action Interval Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Round Type\n",
    "    if 'roundtype' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Round Type Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Timer\n",
    "    if 'timer' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Timer Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Skill Type\n",
    "    if 'skilltype' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Skill Type Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Action Types\n",
    "    if 'actions' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Individual Action Types ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Action Duration\n",
    "    if 'actions_dur' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Individual Action Duration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Collisions\n",
    "    if 'collisions' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Collision Types (Hit, Struck, Tie) ---\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arena Heatmaps - Bot Movement Analysis\n",
    "\n",
    "Visualize bot movement patterns across different game phases (Early, Mid, Late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(arena_heatmaps_output):\n",
    "    # Get all bot directories\n",
    "    bot_dirs = [d for d in os.listdir(arena_heatmaps_output)\n",
    "               if os.path.isdir(os.path.join(arena_heatmaps_output, d))]\n",
    "    \n",
    "    # Sort bot directories by rank from df_sum\n",
    "    if \"Rank\" in df_sum.columns and \"Bot\" in df_sum.columns:\n",
    "        rank_map = df_sum.groupby(\"Bot\")[\"Rank\"].first().to_dict()\n",
    "        bot_dirs = sorted(bot_dirs, key=lambda b: rank_map.get(b, 9999))\n",
    "    else:\n",
    "        bot_dirs = sorted(bot_dirs)\n",
    "    \n",
    "    if bot_dirs:\n",
    "        phase_names = [\"window_2.5-15s.png\", \"window_15-30s.png\", \"window_30-45s.png\", \"window_45-60s.png\"]\n",
    "        \n",
    "        # Display heatmaps for each bot\n",
    "        for bot_name in bot_dirs:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"{bot_name} (#{bot_dirs.index(bot_name)+1})\")\n",
    "            print(f\"{'='*60}\")\n",
    "            bot_dir = os.path.join(arena_heatmaps_output, bot_name)\n",
    "            \n",
    "            # Display phase heatmaps\n",
    "            fig, axes = plt.subplots(1, len(phase_names), figsize=(20, 5))\n",
    "            for idx, phase_name in enumerate(phase_names):\n",
    "                image_path = os.path.join(bot_dir, phase_name)\n",
    "                if os.path.exists(image_path):\n",
    "                    image = Image.open(image_path)\n",
    "                    axes[idx].imshow(image)\n",
    "                    axes[idx].set_title(phase_name)\n",
    "                    axes[idx].axis('off')\n",
    "                else:\n",
    "                    axes[idx].text(0.5, 0.5, f\"Image not found:\\n{phase_name}\",\n",
    "                                  ha='center', va='center')\n",
    "                    axes[idx].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display position distribution\n",
    "            dist_path = os.path.join(bot_dir, \"position_distribution.png\")\n",
    "            if os.path.exists(dist_path):\n",
    "                print(\"\\nPosition Distribution (X & Y Overlayed)\")\n",
    "                dist_image = Image.open(dist_path)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(dist_image)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            \n",
    "            # Display distance distribution\n",
    "            dist_path = os.path.join(bot_dir, \"distance_distribution.png\")\n",
    "            if os.path.exists(dist_path):\n",
    "                print(\"\\nDistance Distribution\")\n",
    "                dist_image = Image.open(dist_path)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(dist_image)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "            print(\"\\nFull Configuration Analysis\")\n",
    "            fig = plot_full_cross_heatmap_half(df, bot_name=bot_name)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No bot heatmaps found in directory\")\n",
    "        print(\"Run: `python detailed_analyzer.py all` to generate heatmaps\")\n",
    "else:\n",
    "    print(f\"Heatmap directory not found: {arena_heatmaps_output}\")\n",
    "    print(\"Run: `python detailed_analyzer.py all` to generate heatmaps for all bots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
